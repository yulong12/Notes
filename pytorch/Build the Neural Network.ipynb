{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649b1ff8-96ba-4c1a-b186-5e4346498411",
   "metadata": {},
   "source": [
    "## Build the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e46611-3cc0-4696-aa31-8c81a07ffe7d",
   "metadata": {},
   "source": [
    "神经网络由对数据执行操作的`layers/modules`组成。`torch.nn `命名空间提供了构建自己的神经网络所需的所有构建块。PyTorch 中的每个模块都是 `nn.Module` 的子类。神经网络本身就是一个由其他模块（层）组成的模块。这种嵌套结构允许轻松构建和管理复杂的架构。   \n",
    "在下面的部分中，我们将构建一个神经网络来对FashionMNIST数据集中的图像进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a5a12c-65a9-4597-9458-fd9861b4cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a691e74a-04c6-482c-b19a-dd59253579e3",
   "metadata": {},
   "source": [
    "#### 获取训练设备类型\n",
    "我们希望能够在 GPU 或 MPS 等硬件加速器上训练我们的模型（如果可用）。让我们检查一下 `torch.cuda `或 `torch.backends.mps` 是否可用，否则我们使用 CPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f52dc56-9437-4f22-a504-bb22aece47cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaaf27f-ee06-4062-83ee-10bda8698239",
   "metadata": {},
   "source": [
    "### 定义类\n",
    "我们通过继承`nn.Module` 来定义我们的神经网络，并在` __init__ `中初始化神经网络层。每个 `nn.Module` 子类都在 `forward` 方法中实现对输入数据的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c1566ce-9c2f-492d-90cd-e963e2c7510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d61f29-6486-4589-b649-341133c3d664",
   "metadata": {},
   "source": [
    "我们创建一个NeuralNetwork的实例，并将其移动到设备上，并打印其结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f282a8ba-971b-4e24-b3a7-991c1e6fd199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82145566-aa50-401a-ac7e-c63bda1e0a87",
   "metadata": {},
   "source": [
    "为了使用该模型，我们将输入数据传递给它。这将执行模型的`forward`，以及一些`background operations`。不要直接调用`model.forward()` ！\n",
    "在输入上调用模型会返回一个二维张量，其中 dim=0 对应于每个类的 10 个原始预测值的每个输出，dim=1 对应于每个输出的单个值。我们通过将其传递给 `nn.Softmax` 模块的一个实例来获得预测概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5210a6f8-2562-4659-9234-bbcaca5ea066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([4], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8689cc-28bc-46f8-a963-31871a98f28b",
   "metadata": {},
   "source": [
    "### 模型layers\n",
    "分解讲解，我们将采用3张大小为28x28的图像的样本minibatch，看看当我们将其通过网络时发生了什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba0af851-bdb5-43eb-86ad-2403789f883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6913ea6-a761-422f-ae85-088b3c669ff0",
   "metadata": {},
   "source": [
    "#### nn.Flatten\n",
    "我们初始化 nn.Flatten 层以将每个 2D 28x28 图像转换为 784 个像素值的连续数组（保持小批量维度（在 dim=0 时））。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f42980c6-a58c-492c-b8a1-b95e5a9d517e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5091b-e35a-46e3-977b-5a7e30f7879c",
   "metadata": {},
   "source": [
    "#### nn.Linear\n",
    "线性层是一个使用其存储的权重和偏差对输入应用线性变换的模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc8baf17-4224-426f-bbb2-f54b0169cc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86038b9e-3f4c-4867-893d-b187bc1883f7",
   "metadata": {},
   "source": [
    "#### nn.ReLU\n",
    "非线性激活会在模型的输入和输出之间创建复杂的映射。它们应用于线性变换后引入非线性，帮助神经网络学习各种各样的现象。\n",
    "在这个模型中，我们在线性层之间使用 nn.ReLU，但是还有其他激活函数可以在模型中引入非线性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a881cb45-8f9c-4b05-bf64-3f8b2743ddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 4.4339e-01, -3.2614e-01,  5.8222e-01,  2.1744e-01, -2.2511e-02,\n",
      "         -4.0086e-02,  4.4455e-01, -1.9883e-01, -8.3516e-01,  6.6676e-02,\n",
      "          2.7168e-02,  6.7465e-01, -4.2861e-01, -1.2839e-01, -2.8343e-01,\n",
      "         -3.6913e-01, -1.9985e-01,  2.3776e-01,  2.6889e-02, -1.0427e-01],\n",
      "        [ 8.9785e-01, -1.9714e-01,  7.6758e-01, -3.1283e-01,  1.8260e-01,\n",
      "         -1.9110e-01,  2.2371e-01, -1.9524e-01, -3.8801e-01,  3.3155e-01,\n",
      "          9.0025e-02,  8.2888e-01,  5.2110e-02, -1.5870e-01,  2.4729e-01,\n",
      "         -2.1455e-01,  2.0598e-01, -8.4822e-02, -3.4520e-01,  5.7028e-02],\n",
      "        [ 6.9799e-01, -8.2192e-03,  9.6877e-01, -3.1217e-01,  4.7034e-01,\n",
      "         -6.1865e-01,  5.7541e-02, -4.5347e-01, -4.9228e-01,  2.9391e-01,\n",
      "          2.3305e-01,  4.4044e-01, -5.1194e-01, -2.6624e-01, -2.8040e-01,\n",
      "          2.4426e-01,  2.6523e-02,  1.1766e-01, -2.4783e-01,  5.7869e-04]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[4.4339e-01, 0.0000e+00, 5.8222e-01, 2.1744e-01, 0.0000e+00, 0.0000e+00,\n",
      "         4.4455e-01, 0.0000e+00, 0.0000e+00, 6.6676e-02, 2.7168e-02, 6.7465e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3776e-01,\n",
      "         2.6889e-02, 0.0000e+00],\n",
      "        [8.9785e-01, 0.0000e+00, 7.6758e-01, 0.0000e+00, 1.8260e-01, 0.0000e+00,\n",
      "         2.2371e-01, 0.0000e+00, 0.0000e+00, 3.3155e-01, 9.0025e-02, 8.2888e-01,\n",
      "         5.2110e-02, 0.0000e+00, 2.4729e-01, 0.0000e+00, 2.0598e-01, 0.0000e+00,\n",
      "         0.0000e+00, 5.7028e-02],\n",
      "        [6.9799e-01, 0.0000e+00, 9.6877e-01, 0.0000e+00, 4.7034e-01, 0.0000e+00,\n",
      "         5.7541e-02, 0.0000e+00, 0.0000e+00, 2.9391e-01, 2.3305e-01, 4.4044e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4426e-01, 2.6523e-02, 1.1766e-01,\n",
      "         0.0000e+00, 5.7869e-04]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e0044-fcbd-44a4-a32d-4821eafce3b1",
   "metadata": {},
   "source": [
    "#### nn.Sequential\n",
    "nn.Sequential 是模块的有序容器。数据按照定义的顺序传递到所有模块。您可以使用顺序容器来快速组合网络，如 `seq_modules`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4e96761-05c7-41ce-8875-125c5cb27b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baae81a0-e14c-47f5-8539-053c8355f2f4",
   "metadata": {},
   "source": [
    "#### nn.Softmax\n",
    "神经网络的最后一层线性层返回 logits - [-infty, infty] 中的原始值,并将其传递给 nn.Softmax 模块。logits 被缩放到 [0, 1] 的值，表示模型对每个类的预测概率。dim 参数表示值必须总和为 1 的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4ee5526-b53f-4d65-9cb0-6a718baaeaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d247b-ec15-487c-8265-b2330164a926",
   "metadata": {},
   "source": [
    "#### 模型参数\n",
    "神经网络中的许多层都是参数化的，即具有在训练期间优化的相关weights和biases。子类化 nn.Module 会自动跟踪模型对象内定义的所有字段，并使用模型的 `parameters() `或 `named_parameters() `方法访问所有参数。\n",
    "在这个例子中，我们迭代每个参数，并打印它的大小和它的值的预览。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "844511a8-54b7-4f10-9f36-ef910e598da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0113,  0.0091, -0.0320,  ...,  0.0157,  0.0041,  0.0139],\n",
      "        [-0.0244,  0.0239,  0.0219,  ..., -0.0209,  0.0016, -0.0056]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0293,  0.0196], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0167, -0.0233, -0.0418,  ...,  0.0247, -0.0302, -0.0344],\n",
      "        [-0.0321,  0.0374,  0.0405,  ..., -0.0254, -0.0237, -0.0412]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0392, -0.0337], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0441,  0.0382,  0.0131,  ..., -0.0423,  0.0418, -0.0047],\n",
      "        [ 0.0196,  0.0179, -0.0123,  ..., -0.0077,  0.0320, -0.0111]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0037, -0.0081], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c1151-20f1-41ca-b7b4-b50d0ed7e3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
